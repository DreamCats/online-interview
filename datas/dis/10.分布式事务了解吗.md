# 分布式事务

> 人只能扔掉真正属于他自己的东西。
> 两个人如果读过同一本书，他们之间就有了一条纽带。

## 背景

CAP 理论暂时先不讨论，姑且就单论一致性来说，对于关系型数据库我们通常利用**事务**来保证数据的强一致性。

但是当我们的数据量越来越大，大到单库已经无法承担时，我们不得不采取**分库分表**的策略对数据库实现水平拆分，或者引入 NoSQL 技术，构建分布式数据库集群以分摊读写压力，从而提升数据库的存储和响应能力。

但是多个数据库实例也为我们使用数据库带来了许多的限制，比如主键的全局唯一、联表查询、数据聚合等等，另外一个相当棘手的问题就是数据库的事务由原先的**单库事务**变成了现在的**分布式事务**。

分布式事务的实现并不是无解的，比如**两阶段提交**（2PC：Two-Phase Commit）和**三阶段提交**（3PC：Three-Phase Commit）都给我们提供了思路，但是在分布式环境下如何保证数据的强一致性，并对外提供高可用的服务还是相当棘手的，因此很多分布式系统对于数据强一致性都敬而远之。

## 两阶段提交

为什么会有两阶段提交呢？

假如，目前有一个单机系统架构，当在一个服务中调用多个服务时，如果出现了异常，此时由于单机的情况下，可以使用单库提供的事务回滚特性，可以解决一致性问题，但是如果不是单机系统呢？多个服务在多个节点上，多个库在多个节点上，那么假设出现了异常，如何告知服务需要回滚呢？

这个时候可能需要一个全局管理者，管理多个服务的提交与回滚。那么，既然提到了管理者，如果是你，你怎么设计流程呢？可能联想到是这样的场景：

1. 管理者告知 A 服务和 B 服务，你们分别执行自己的服务，提交事务前分别告知我一声，让我方便统计一下是不是都可以正常进行。（第一阶段）
2. 开始统计服务回馈给自己的票数，如果全是 ok，那么发号施令，你们可以提交了；但凡有一个票数不是 ok，那么发号施令，你们全部都得回滚。（第二阶段）

所以，我们可以把上面的 A 服务和 B 服务，称为**参与者**（额...专业点），将管理者称为**协调者**。

上述第一阶段可以称为**投票**，第二阶段**提交事务**。

### 第一阶段

该阶段的主要目的在于打探数据库集群中的各个参与者是否能够正常的执行事务，具体步骤如下：

1. 协调者向所有的参与者发送**事务执行请求**，并等待参与者反馈事务执行结果；
2. 事务参与者收到请求之后，**执行事务但不提交，并记录事务日志**；
3. 参与者将自己事务执行情况反馈给协调者，**同时阻塞等待协调者的后续指令**。

### 第二阶段

在经过第一阶段协调者的询盘之后，各个参与者会回复自己事务的执行情况，这时候存在 3 种可能性：

1. 所有的参与者都回复能够**正常执行事务**；
2. 一个或多个参与者回复**事务执行失败**；
3. 协调者**等待超时**。

对于第 1 种情况，协调者将向所有的参与者发出提交事务的通知，具体步骤如下：

1. 协调者向各个参与者**发送 commit 通知**，请求提交事务；
2. 参与者收到事务提交通知之后执行 commit 操作，然后释放占有的资源；
3. 参与者向协调者返回事务 commit 结果信息。

![](https://imgs.heiye.site/byte/1648623246803.png)

对于第 2 和第 3 种情况，协调者均认为参与者无法成功执行事务，为了整个集群数据的一致性，所以要向各个参与者发送事务回滚通知，具体步骤如下：

1. 协调者向各个参与者发送**事务 rollback 通知**，请求回滚事务；
2. 参与者收到事务回滚通知之后执行 rollback 操作，然后释放占有的资源；
3. 参与者向协调者返回事务 rollback 结果信息。

![](https://imgs.heiye.site/byte/1648623368689.png)

所以两阶段呢，是来解决分布式数据库中事务一致性问题，但实际应用中更多是用来解决原子性问题。

优点：简单，易实现；

缺点：

- **单点故障**：

协调者在整个两阶段提交过程中扮演着举足轻重的作用，**一旦协调者所在服务器宕机**，就会影响整个数据库集群的正常运行。比如在第二阶段中，如果协调者**因为故障不能正常发送事务提交或回滚通知**，那么参与者们将**一直处于阻塞状态**，整个数据库集群将无法提供服务。

- **同步阻塞**：

两阶段提交执行过程中，所有的参与者都需要听从协调者的统一调度，期间**处于阻塞状态而不能从事其他操作**，这样效率极其低下。比如，有些服务返回通知较快，有些服务则较慢，较快的服务则一直在等待。

- **数据不一致**：

两阶段提交协议虽然是分布式数据强一致性所设计，但仍然存在数据不一致性的可能性。比如在第二阶段中，假设协调者发出了事务 commit 通知，但是因为**网络问题该通知仅被一部分参与者所收到并执行了 commit 操作**，其余的参与者则因为**没有收到通知一直处于阻塞状态**，这时候就产生了数据的不一致性。

如何解决以上问题呢，那么从问题着手，可设计**超时机制**和**互相轮询机制**。

1. 站在协调者的角度，如果在指定时间内没有收到所有参与者的应答，则可以自动退出 WAIT 状态，并向所有参与者发送 rollback 通知。
2. 站在参与者的角度，如果在指定时间内没有收到协调者的通知，这个时候不能执行 RollBack，因为可能协调者发送的是 Commit 通知，可能由于网络阻塞，自己没有收到，那么此时可能需要使用刚才提到的**互相轮询**，询问一下其他服务收到的通知类型，此时又出现几种情况：
   1. 如果其他服务，比如 B，它未处于 Ready 状态，说明协调者等 B，等的发慌，B 自己在第一阶段执行太久了，超时了呗，那么协调者等超时了，就发送 RollBack 了，那自己可以放心的去执行 RollBack 命令。
   2. 如果 B 服务处于 Ready 状态，并且在执行 RollBack 或者 Commit 命令，那么自己也可以执行相同的命令。
   3. 如果 B 服务处于 Ready 状态，也没有做任何动作，它可能和自己一样，走的是 2g 网...，那这个时候继续找 C，找 D，直到所有都是像自己一样都是没网了。那可能不是自己没网了，而是你协调者可能出事了。

那怎么办呢？

## 三阶段

针对两阶段提交存在的问题，三阶段提交协议通过引入一个**预询盘**阶段，以及**超时策略**来减少整个集群的阻塞时间，提升系统性能。三阶段提交的三个阶段分别为：预询盘（can_commit）、预提交（pre_commit），以及事务提交（do_commit）。

### 第一阶段

该阶段协调者会去询问各个参与者是否能够正常执行事务，参与者根据自身情况回复一个**预估值**（个人感觉是衡量一下网络阻塞的情况），相对于真正的执行事务，这个过程是**轻量的**，具体步骤如下：

1. 协调者向各个参与者发送事务询问通知，询问是否可以执行事务操作，并等待回复；（检查一下通不通）
2. 各个参与者依据自身状况回复**一个预估值**，如果预估自己能够正常执行事务就返回确定信息，并进入预备状态，否则返回否定信息。（全部畅通无阻，基本不怎么超时，放心来吧）

### 第二阶段

本阶段协调者会根据第一阶段的询盘结果采取相应操作，询盘结果主要有 3 种：

1. 所有的参与者都返回确定信息；（网络通）
2. 一个或多个参与者返回否定信息；
3. 协调者等待超时。（协调者还是要等...）

针对第 1 种情况，协调者会向所有参与者发送事务执行请求，具体步骤如下：（你可以理解为和两阶段的第一阶段类似）

1. 协调者向所有的事务参与者发送事务执行通知；
2. 参与者收到通知后执行事务但不提交；
3. 参与者将事务执行情况返回给协调者。

但与此不同的是：**如果参与者等待超时，则会中断事务，不会一直等**；而对于第一阶段的 2、3 结果，协调者直接发送 abort 通知，直接退出，不用在占用资源。

1. 协调者向所有事务参与者发送 abort 通知；
2. 参与者收到通知后中断事务。

### 第三阶段

如果第二阶段事务未中断，那么本阶段协调者将会依据事务执行返回的结果来决定提交或回滚事务，分为 3 种情况：（和两阶段的第二阶段类似）

1. 所有的参与者都能正常执行事务。
2. 一个或多个参与者执行事务失败。
3. 协调者等待超时。

第一种情况，无非是协调者继续给参与者发送 commit，关于过程和结果不再介绍；

第二、三种情况，协调者给参与者发送 RollBack，过于过程和结果不再介绍；

不过，在第三阶段中，如果因为协调者或网络问题，导致**参与者迟迟不能收到来自协调者的 commit 或 rollback 请求**，那么参与者将不会如两阶段提交中那样陷入阻塞，而是等待超时后**继续 commit**，相对于两阶段提交虽然降低了同步阻塞，**但仍然无法完全避免数据的不一致**。

三阶段相对于两阶段提交，主题是降低了**超时**，从参与者角度降低了超时，三阶段的**第一阶段非常轻量级的去校验一下网络等其他参数，尽量少占用点资源**。

不过，二阶段和三阶段，再锁资源这一方面，粒度还是较大的，比如，有这样的场景：用户在电商网站购买商品 1000 元，使用余额支付 800 元，使用红包支付 200 元。我们看一下在 2PC 中的流程：

- prepare 阶段：

1. 下单系统插入一条订单记录，不提交；
2. 余额系统减 800 元，给记录加锁（锁记录），写 redo 和 undo 日志，不提交；
3. 红包系统减 200 元，给记录加锁（锁记录），写 redo 和 undo 日志，不提交；

- commit 阶段：

1. 下单系统提交订单记录
2. 余额系统提交，释放锁
3. 红包系统提交，释放锁

为什么说这是一种大粒度的资源锁定呢？是因为在 prepare 阶段，当数据库给用户余额减 800 元之后，为了维持隔离性，**会给该条记录加锁**，在事务提交前，其它事务无法再访问该条记录。但实际上，**我们只需要预留其中的 800 元，不需要锁定整个用户余额**。这是 2PC 和 3PC 的局限，因为这两者是**资源层**的协议，无法提供更灵活的资源锁定操作。

所以，TCC（try-confirm-cancel） 应运而生。

## TCC

TCC 本质上也是一个二阶段提交协议，TCC 将事务的提交过程分为 try-confirm-cancel(实际上 TCC 就是 try、confirm、cancel 的简称) 三个阶段:

1. try：完成业务检查、预留业务资源；
2. confirm：使用预留的资源执行业务操作（需要保证幂等性）；
3. cancel：取消执行业务操作，释放预留的资源（需要保证幂等性）。

这三步就不再详细介绍了，我们尝试 TCC 走一下刚才的场景：

- Try 阶段

1. tryX 下单系统创建待支付订单
2. tryY 冻结账户红包 200 元
3. tryZ 冻结资金账户 800 元

- Confirm 操作

1. confirmX 订单更新为支付成功
2. confirmY 扣减账户红包 200 元
3. confirmZ 扣减资金账户 800 元

- Cancel 操作

1. cancelX 订单处理异常，资金红包退回，订单支付失败
2. cancelY 冻结红包失败，账户余额退回，订单支付失败
3. cancelZ 冻结余额失败，账户红包退回，订单支付失败

可以看到，我们使用了冻结代替了原先的账号锁定（实际操作中，**冻结操作可以用数据库减操作+日志实现**），这样在冻结操作之后，事务提交之前，**其它事务也能使用账户余额，提高了并发性**。

总结一下，相比于二阶段提交协议，TCC 主要有以下区别：

1. 2PC 位于**资源层**而 TCC 位于**服务层**。
2. 2PC 的接口由第三方厂商实现，TCC 的接口由开发人员实现。
3. TCC 可以更灵活地控制资源锁定的粒度。
4. TCC 对应用的**侵入性强**。业务逻辑的**每个分支**都需要实现 try、confirm、cancel 三个操作，应用侵入性较强，改造成本高。

## 可靠消息最终一致性

在上面的通用方案设计里，完全依赖可靠消息服务的各种自检机制来确保：

- 如果上游服务的数据库操作没成功，下游服务是不会收到任何通知。
- 如果上游服务的数据库操作成功了，**可靠消息服务死活都会确保将一个调用消息投递给下游服务，而且一定会确保下游服务务必成功处理这条消息**。

通过这套机制，保证了基于 MQ 的异步调用/通知的服务间的分布式事务保障。其实阿里开源的 RocketMQ，就实现了可靠消息服务的所有功能，核心思想跟上面类似。

> 参考：https://segmentfault.com/a/1190000012534071#/
