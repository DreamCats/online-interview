# RocketMQ 的架构初探

> 多少次迎着冷眼与嘲笑
> 从没有放弃过心中的理想
> 一刹那恍惚若有所失的感觉

## 背景

> 在目前的互联网公司中，MQ 还是被广泛的投入与使用，但是，我们为什么要使用消息队列呢？虽然我上一篇文章已经讲过，但是，我们还是需要深入了解一下消息队列的架构，以及它的优缺点。

再次提一下消息队列的特点和场景：

> 消息队列主要解决了应用**耦合**、**异步处理**、**流量削锋**等问题。

## 一个小故事

我们知道，消息队列最简单的架构模型无非是**生产者-消费者模型**，那么我们请看一下此时的架构模型：

![](https://imgs.heiye.site/byte/1644906067851.png)

从图中可以看到几个角色：

- producer：生产者，你可以理解为负责生产任务；
- queue：队列，没有它，就失去了解耦的灵魂，消息队列就没有意义了；
- consumer：既然队列有任务，那么很明显需要被消费，所以消费者的价值体现。

## 分区队列

在我们单体应用程序中，使用此模型，非常简单，且容易理解，但是，有一个问题，如果任务量越来越多，此时一个队列很可能达到上限，那么，我们该如何解决这个问题呢？

很容易想到：**分区队列**，我们可以将任务分配到不同的队列中，每个队列可以设置不同的队列长度，这样，当队列满了，就会将任务放到下一个队列中，这样，就可以解决这个问题了。

![](https://imgs.heiye.site/byte/1644907070350.png)

## 顺序消息

但此时，还会存在一个问题，producer 普通发送消息，暂时没有问题，无非是两种机制：**轮询机制**和**故障规避机制**，在 rocketmq 中默认使用轮询机制，轮询选择其中一个队列。

> 轮询机制的原理是路由信息 TopicPublishInfo 中维护了一个计数器 sendWhichQueue，每发送一次消息需要查询一次路由，计算器就进行“+1”，通过计数器的值 index 与队列的数量取模计算来实现轮询算法。

轮询算法简单好用，但是有个弊端，如果轮询选择的队列是在宕机的 Broker 上，会导致消息发送失败，即使消息发送重试的时候重新选择队列，也可能还是在宕机的 Broker 上，无法规避发送失败的情况，因此就有了故障规避机制。

回到刚才的问题，如果此时有一个场景，保证顺序的应用场景非常多，比如交易场景中的订单创建、支付、退款等流程，先创建订单才能支付，支付完成的订单才能退款，这需要保证先进先出。又例如数据库的 BinLog 消息，数据库执行新增语句、修改语句，BinLog 消息得到顺序也必须保证是新增消息、修改消息。

遇到这样的场景，有什么样的解决方案保证顺序消息呢？

你可能会说：我们将同一个订单的处理流程放到一个队列中，那不就解决了吗？

那怎么才能让同一个订单放到同一个队列中呢？

> RocketMQ 的顺序消息分为 2 种情况：局部有序和全局有序。前面的例子是局部有序场景。
>
> - 局部有序：指发送同一个队列的消息有序，可以在发送消息时指定队列，在消费消息时也按顺序消费。例如同一个订单 ID 的消息要保证有序，不同订单的消息没有约束，相互不影响，不同订单 ID 之间的消息时并行的。
> - 全局有序：设置 Topic 只有一个队列可以实现全局有序，创建 Topic 时手动设置。此类场景极少，性能差，通常不推荐使用。

顺序消息发送的原理比较简单，同一类消息发送到相同的队列即可。为了保证先发送的消息先存储到消息队列，必须使用同步发送的方式，否则可能出现先发送的消息后到消息队列中，此时消息就乱序了。

我们可以思考一下一个数据结构：哈希表，那么哈希表是如何定位对应的 index 呢？

所以 RocketMQ 的也采用了这样的思想：选择队列的过程由 messageQueueSelector 和 hashKey 在实现类 SelectMessageQueueByHash 中完成

```java
public class SelectMessageQueueByHash implements MessageQueueSelector {
    public SelectMessageQueueByHash() {
    }

    public MessageQueue select(List<MessageQueue> mqs, Message msg, Object arg) {
        int value = arg.hashCode();
        if (value < 0) {
            value = Math.abs(value);
        }

        value %= mqs.size();
        return (MessageQueue)mqs.get(value);
    }
}
```

- 根据 hashKey 计算 hash 值，hashKey 是我们前面例子中订单 ID，因此相同订单 ID 的 hash 值相同。
- 用 hash 值和队列数 mqs.size()取模，得到一个索引值，结果小于队列数。
- 根据索引值从队列列表中取出一个队列 mqs.get(value)，hash 值相同则队列相同。

注意：在队列列表的获取过程中，由 Producer 从 NameServer 根据 Topic 查询 Broker 列表，缓存在本地内存中，以便下次从缓存中读取。（**缓存的应用，不仅在 RocketMQ 中有体现，在众多的分布式中间件当中都会存在这样的使用。**）

顺序发送原理已经初步了解，那么顺序消费呢？

> RocketMQ 支持两种消费模式：集群消费（点对点）和广播消费。两者的区别是，在广播消费模式下每条消息会被 ConsumerGroup 的每个 Consumer 消费，在集群消费模式下每条消息只会被 ConsumerGroup 的一个 Consumer 消费。

> 多数场景都使用集群消费，消息每次消费代表一次业务处理，集群消费表示每条消息由业务应用集群中任意一个服务实例来处理。少数场景使用广播消费，例如数据发生变化，更新业务应用集群中每个服务的本地缓存，这就需要一条消息被整个集群都消费一次，默认是集群消费。

顺序消费也叫做有序消费，**原理是同一个消息队列只允许 Consumer 中的一个消费线程拉取消费，Consumer 中有个消费线程池，多个线程会同时消费消息**。在顺序消费的场景下消费线程请求到 Broker 时会先申请独占锁，获得锁的请求则允许消费。

消息消费成功后，会向 Broker 提交消费进度，更新消费位点信息，避免下次拉取到已消费的消息，顺序消费中如果消费线程在监听器中进行业务处理时抛出异常，则不会提交消费进度，消费进度会阻塞在当前这条消息，并不会继续消费该队列中的后续消息，从而保证顺序消费。

![](https://imgs.heiye.site/byte/1644908178305.png)

## 消息幂等

以上也有可能，存在某种情况同一个消费被重复消费，那么如何保证消息幂等呢？

> 任意多次执行所产生的影响均与一次执行的影响相同就可以称为幂等

> 当出现消费者对某条消息重复消费的情况时，重复消费的结果与消费一次的结果是相同的，并且多次消费并未对业务系统产生任何负面影响

消息队列无法保证幂等，此情况得基于**业务场景**进行考量。

此处简单提供一些幂等方案：

1. 利用数据库的唯一约束实现幂等。

> 比如将订单表中的订单编号设置为唯一索引，创建订单时，根据订单编号就可以博阿正幂等

2. 去重表

> 这个方案本质也是根据数据库的唯一性约束来实现。其实现大体思路是：首先在去重表上建唯一索引，其次操作时把业务表和去重表放在同个本地事务中，如果出现重现重复消费，数据库会抛唯一约束异常，操作就会回滚

3. 利用 Redis 的原子性

> 每次操作都直接 set 到 redis 里面，然后将 redis 数据定时同步到数据库中

4. 多版本（乐观锁）控制

> 此方案多用于更新的场景下。其实现的大体思路是：给业务数据增加一个版本号属性，每次更新数据前，比较当前数据的版本号是否和消息中的版本一致，如果不一致则拒绝更新数据，更新数据的同时将版本号+1

5. 状态机机制

> 此方案多用于更新且业务场景存在多种状态流转的场景

6. token 机制

> 生产者发送每条数据的时候，增加一个全局唯一的 id，这个 id 通常是业务的唯一标识，比如订单编号。在消费端消费时，则验证该 id 是否被消费过，如果还没消费过，则进行业务处理。处理结束后，在把该 id 存入 redis，同时设置状态为已消费。如果已经消费过了，则不进行处理。

## RocketMQ 架构

好了，如果此时体量越来越大，你需要存储到不同的机器上做分布式，那么想做成分布式，就需要考虑几个问题了。

- producer 是否可集群可管理
- consumer 是否可集群可管理
- queue 是否可集群可管理

此时，你可能想到需要一个分布式治理服务的中间件，类似于 zk 或者 etcd，这样就可以把消息队列和消费者管理分布式了。

![](https://imgs.heiye.site/byte/1644909225111.png)

### 角色介绍

简单介绍一下角色：

- Producer：消息发布的角色，支持分布式集群方式部署。Producer 通过 MQ 的负载均衡模块选择相应的 Broker 集群队列进行消息投递，投递的过程支持快速失败并且低延迟。
- Consumer：消息消费的角色，支持分布式集群方式部署。支持以 push 推，pull 拉两种模式对消息进行消费。同时也支持集群方式和广播方式的消费，它提供实时消息订阅机制，可以满足大多数用户的需求。
- NameServer：NameServer 是一个非常简单的 Topic 路由注册中心，其角色类似 Dubbo 中的 zookeeper，支持 Broker 的动态注册与发现。主要包括两个功能：Broker 管理，NameServer 接受 Broker 集群的注册信息并且保存下来作为路由信息的基本数据。然后提供**心跳检测机制**，检查 Broker 是否还存活;**路由信息管理**，每个 NameServer 将保存关于 Broker 集群的整个路由信息和用于客户端查询的队列信息。然后 Producer 和 Conumser 通过 NameServer 就可以知道整个 Broker 集群的路由信息，从而进行消息的投递和消费。**NameServer 通常也是集群的方式部署，各实例间相互不进行信息通讯**。Broker 是向每一台 NameServer 注册自己的路由信息，所以每一个 NameServer 实例上面都保存一份完整的路由信息。当某个 NameServer 因某种原因下线了，Broker 仍然可以向其它 NameServer 同步其路由信息，Producer,Consumer 仍然可以动态感知 Broker 的路由的信息。**但是 NameServer 并不会像 ZK 一样提供选举功能。**
- BrokerServer：Broker 主要负责消息的存储、投递和查询以及服务高可用保证，为了实现这些功能，Broker 包含了以下几个重要子模块。
  - Remoting Module：整个 Broker 的实体，负责处理来自 clients 端的请求。
  - Client Manager：负责管理客户端(Producer/Consumer)和维护 Consumer 的 Topic 订阅信息
  - Store Service：提供方便简单的 API 接口处理消息存储到物理硬盘和查询功能。
  - HA Service：高可用服务，提供 Master Broker 和 Slave Broker 之间的数据同步功能。
  - Index Service：根据特定的 Message key 对投递到 Broker 的消息进行索引服务，以提供消息的快速查询。

> Producer 与 NameServer 集群中的其中一个节点(随机选择)建立长连接，定期从 NameServer 获取 Topic 路由信息，并向提供 Topic 服务的 Master 建立长连接，且定时向 Master 发送心跳。Producer 完全无状态，可集群部署。

> Consumer 与 NameServer 集群中的其中一个节点(随机选择)建立长连接，定期从 NameServer 获取 Topic 路由信息，并向提供 Topic 服务的 Master、Slave 建立长连接，且定时向 Master、Slave 发送心跳。Consumer 既可以从 Master 订阅消息，也可以从 Slave 订阅消息，消费者在向 Master 拉取消息时，Master 服务器会根据拉取偏移量与最大偏移量的距离(判断是否读老消息，产生读 I/O)，以及从服务器是否可读等因素建议下一次是从 Master 还是 Slave 拉取。

### 启动流程

RocketMQ 架构启动流程：

- 启动 NameServer，NameServer 起来后监听端口，等待 Broker、Producer、Consumer 连上来，相当于一个路由控制中心。
- Broker 启动，跟所有的 NameServer 保持长连接，定时发送心跳包。心跳包中包含当前 Broker 信息(IP+端口等)以及存储所有 Topic 信息。注册成功后，NameServer 集群中就有 Topic 跟 Broker 的映射关系。
- 收发消息前，先创建 Topic，创建 Topic 时需要指定该 Topic 要存储在哪些 Broker 上，也可以在发送消息时自动创建 Topic。
- Producer 发送消息，启动时先跟 NameServer 集群中的其中一台建立长连接，并从 NameServer 中获取当前发送的 Topic 存在哪些 Broker 上，轮询从队列列表中选择一个队列，然后与队列所在的 Broker 建立长连接从而向 Broker 发消息。
- Consumer 跟 Producer 类似，跟其中一台 NameServer 建立长连接，获取当前订阅 Topic 存在哪些 Broker 上，然后直接跟 Broker 建立连接通道，开始消费消息。

### 完整的模型

![](https://imgs.heiye.site/byte/1644913605554.png)

我们在图中发现消费者获取消息有两种方式：推和拉

### 推拉模式

消费者客户端有两种方式从消息中间件获取消息并消费。严格意义上来讲，RocketMQ 并没有实现 Push 模式，而是对**拉模式进行一层包装**，名字虽然是 Push 开头，实际在实现时，使用 Pull 方式实现。**通过 Pull 不断轮询 Broker 获取消息**。当不存在新消息时，Broker 会挂起请求，直到有新消息产生，取消挂起，返回新消息。

Push 模式可以说基于订阅与发布模式，而 Pull 模式可以说是基于消息队列模式。

Pull：

- 消费者向 broker 拉取消息时，如果消息未到达消费队列，并且**未启用长轮询机制**，则会在服务端等待 shortPollingTimeMills(默认 1 秒) 时间后再去判断消息是否已经到达消息队列，如果消息未到达，则提示消息拉取客户端 PULL_NOT_FOUND。
- 如果开启**长轮询模式**，rocketMQ 会每 5s 轮询检查一次消息是否可达，同时一有新消息到达后立马通知挂起线程再次验证新消息是否是自己感兴趣的消息，如果是则从 commitlog 文件提取消息返回给消息拉取客户端，否则直到挂起超时，超时时间由消息拉取方在消息拉取时封装在请求参数中，PUSH 模式默认 15s。

Push：

- 后台独立线程 RebalanceServic 根据 Topic 中消息队列个数和当前消费组内消费者个数进行负载均衡，给当前消费者分配对应的 MessageQueue，将其封装为 PullRequest 实例放入队列 pullRequestQueue 中。
- Consumer 端开启后台独立的线程 PullMessageService 不断地从队列 pullRequestQueue 中获取 PullRequest 并通过网络通信模块异步发送 Pull 消息的 RPC 请求给 Broker 端。这里算是比较典型的生产者-消费者模型，实现了准实时的自动消息拉取。
- PullMessageService 异步拉取到消息后，通过 PullCallback 进行回调处理，如果拉取成功，则更新消费进度，putPullRequest 到阻塞队列 pullRequestQueue 中，接着立即进行拉取。
- 监听器 ConsumeMessageConcurrentlyService 会一直监听回调方法 PullCallback，把拉取到的消息交给 Consumerrequest 进行处理，Consumerrequest 会调用消费者业务方实现的 consumeMessage()接口处理具体业务，消费者业务方处理完成后返回 ACK 给 Consumerrequest，如果消费者 ACK 返回的失败，则在集群模式下把消息发回 Broker 进行重试(广播模型重试的成本太高)，最后更新消费进度 offsetTable。
- 在 Broker 端，PullMessageProcessor 业务处理器收到 Pull 消息的 RPC 请求后，通过 MessageStore 实例从 commitLog 获取消息。如果第一次尝试 Pull 消息失败(比如 Broker 端没有可以消费的消息)，则通过长轮询机制先 hold 住并且挂起该请求，然后通过 Broker 端的后台线程 PullRequestHoldService 重新尝试和后台线程 ReputMessageService 进行二次处理。

![](https://imgs.heiye.site/byte/1644915102148.png)

简单介绍一下长轮询和短轮询：

短轮询：定时发起请求，服务端收到请求后不论数据有没有更新都立即返回
优点：实现简单，容易理解
缺点：服务端是被动的，服务端要不断的处理客户端连接，并且服务端无法控制客户端 pull 的频率以及客户端数量。

长轮询：对普通轮询的优化，依然由客户端发起请求，服务端收到后并不立即响应而是 hold 住客户端连接，等待数据产生变更后(或者超过指定时间还未产生变更)才回复客户端。

关于 Broker 对消息的存储，RocketMQ 做了一定的优化。

### 存储设计

存储结构：

![](https://imgs.heiye.site/byte/1644916024225.png)

- Commitlog 文件

当生产者将消息发送到 RocketMQ 的 Broker 之后，需要将消息进行持久化存储，防止消息数据丢失。RocketMQ 将消息数据写入存储文件 CommitLog 中，按照消息的发送顺序写入文件当中，每个文件的大小约为 1G，当达到文件大小限制后，就会创建新的 CommitLog 文件。（物理地址）

- ConsumerQueue 文件

CommitLog 文件中的消息数据是一条一条顺序写的，最笨的方法就是遍历文件，作为一款高性能的消息中间件，显然这不是一个好的解决方案。

那么我们可不可以借助数据库提升数据查询的方式，使用索引来加快消息数据的查询呢？答案是肯定的。就像 Mysql 中的索引本身需要文件保存一样，在 RocketMQ 中页有单独保存索引的文件，就是 ConsumerQueue 文件。（逻辑队列）

- Index 文件

RocketMQ 的特性功能就是可以实现按照消息的属性进行消息搜索，即建立了索引 Key 的 hashcode 与物理偏移量的映射关系，根据 key 先快速定义到 commitlog 文件。

存储运作流程：

![](https://imgs.heiye.site/byte/1644915899303.png)

1. 生产者生产消息，写入 CommitLog 文件，异步将消息写入到 ConsumerQueue 文件中；
2. 消费者首先通过 ConsumerQueue 文件获取消息的物理地址，最后通过 ComitLog 文件获取消息的数据。

关于其他存储结构的一些优化，后期另出文章讲解。

## 小结

重要的是 RocketMQ 怎么围绕这三点来设计的：

1. 高并发
2. 高可用
3. 高性能

## 参考

- [https://developer.51cto.com/article/671208.html](https://developer.51cto.com/article/671208.html)
- [https://xie.infoq.cn/article/9aade37d8e85207a9ec8083d8](https://xie.infoq.cn/article/9aade37d8e85207a9ec8083d8)
